<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron, Deepak Pathak, Saurabh Gupta and Aditya Kusupati*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
     font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
     font-size: 16px;
     font-weight: 400
  }
  heading {
     font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
     font-size: 19px;
     font-weight: 1000
  }
  strong {
     font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
     font-size: 16px;
     font-weight: 800
  }
  strongred {
     font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
     color: 'red' ;
     font-size: 16px
  }
  sectionheading {
     font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
     font-size: 22px;
     font-weight: 600
  }
  pageheading {
     font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
     font-size: 38px;
     font-weight: 400
  }
  </style>
  <!-- <link rel="icon" type="image/png" href="images/W.png"> -->
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Ranajoy Sadhukhan</title>
  <meta name="Ranajoy Sadhukhan's Homepage" http-equiv="Content-Type" content="Ranajoy Sadhukhan's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
<table width="1200" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
     <pageheading>Ranajoy Sadhukhan</pageheading><br>
     <b>email</b>:
     <font id="email" style="display:inline;">
        <noscript><i>Please enable Javascript to view</i></noscript>
     </font>
     <script>
     emailScramble = new scrambledString(document.getElementById('email'),
          'emailScramble', 'rsadhukhan900@gmail.com',
          [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]);
     </script>
  </p>

  <tr>
     <td width="32%" valign="top"><a href="#Bio"><img src="images/portrait.jpeg" width="100%" style="border-radius:15px"></a>
     <p align=center>
     <a href="pubs/CV.pdf" target="_blank">CV</a> | <a href="https://scholar.google.com/citations?hl=en&user=okWY7CwAAAAJ" target="_blank">Scholar</a> | <a href="https://github.com/ranonrkm" target="_blank">Github</a><br>
     </p>
     </td>
     <td width="68%" valign="top" align="justify" id="Bio">
     <p> I am a Ph.D. student at <a href="https://keroro824.github.io/lab-page/" target="_blank">InfiniAI Lab</a> with <a href="https://www.andrew.cmu.edu/user/beidic/" target="_blank">Prof. Beidi Chen</a> at Carnegie Mellon University. I am broadly interested in developing algorithms and system optimizations for efficient Machine Learning. Currently, I am working on <strong>test-time scaling laws</strong> for Large Language Models and how it can be leveraged to achieve higher training throughput for Reinforcement Learning. Additionally, I am keen on developing <strong>memory-augmented transformer models</strong> with scalable knowledge bank decoupled from a lightweight reasoning backbone model. 
     </p>
    <p> I am currently working as a research intern at <a href="https://ai.meta.com/" target="_blank">Meta</a> with <a href="https://yuandong-tian.com/" target="_blank">Dr. Yuandong Tian</a>, <a href="https://zechunliu.com/" target="_blank">Dr. Zechun Liu</a> and <a href="https://scholar.google.com/citations?user=_4a7rAsAAAAJ&hl=en" target="_blank">Rick Cao</a> on novel transformer architectures with scalable and interpretable knowledge capacity.
    <p>
    Previously, I have worked as a research fellow at Microsoft Research India. I am fortunate to be advised by <a href="https://harsha-simhadri.org/" target="_blank">Dr. Harsha Vardhan Simhadri</a> and <a href="http://manikvarma.org/" target="_blank">Dr. Manik Varma</a>. At MSR, I have worked on devising efficient and robust solutions for large-scale retrieval and recommendation system using memory-efficient and low-latency approximate nearest neighbor search methods (<a href="https://github.com/microsoft/DiskANN" target="_blank">DiskANN</a>). Additionally, I collaborated with the <a href="https://www.microsoft.com/en-us/research/project/extreme-classification/" target="_blank">Extreme Classification</a> team.</p>
     <p> I graduated from the Indian Institute of Technology Kharagpur with a dual degree (5 yr B. Tech. + M. Tech.) in Electrical Engineering and a minor in Computer Science Engineering. At IIT Kharagpur, I had the privilege of working with <a href="http://www.facweb.iitkgp.ac.in/~jay/" target="_blank">Dr. Jayanta Mukhopadhyay</a> on my thesis projects focused on interpretable and robust model optimization for image classification.</p>
     </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td id="Publications"> <sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
     <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2506.05333" target="_blank"><img src="images/kinetics.png" alt="kinetics" width="90%" height="250px" style="border-radius:15px"></a>
     <td width="67%" valign="top">
         <p><a href="https://arxiv.org/abs/2506.05333" target="_blank">
         <heading>Kinetics: Rethinking Test-Time Scaling Laws</heading></a><br>
         <strong>Ranajoy Sadhukhan</strong>, Zhuoming Chen, Haizhong Zheng, Yang Zhou, Emma Strubell, Beidi Chen<br>
         Neural Information Processing Systems (<strong>NeurIPS</strong>), 2025<br>
         TTODLER25 Best Paper Award, LCFM25 Oral, KDD GenAI25 Oral<br>
         </p>

         <div class="paper" id="kinetics">
         <a href="javascript:toggleblock('kineticsabs')">abstract</a> / 
         <a shape="rect" href="javascript:togglebib('kinetics')" class="togglebib">bibtex</a> / 
         <a href="https://arxiv.org/abs/2506.05333" target="_blank">paper</a>
         / <a href="https://infini-ai-lab.github.io/Kinetics/" target="_blank">blog</a>
         <br>

         <p align="justify"> <i id="kineticsabs">We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential and increasingly important with more computing invested, for realizing the full potential of test-time scaling where, unlike training, accuracy has yet to saturate as a function of computation, and continues to improve through increased generation.</i></p>

<pre xml:space="preserve">
  @misc{sadhukhan2025kineticsrethinkingtesttimescaling,
    title={Kinetics: Rethinking Test-Time Scaling Laws}, 
    author={Ranajoy Sadhukhan and Zhuoming Chen and Haizhong Zheng and Yang Zhou and Emma Strubell and Beidi Chen},
    year={2025},
    eprint={2506.05333},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2506.05333}, 
}
</pre>
        </div>
      </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
     <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2410.16179" target="_blank"><img src="images/magicpig.png" alt="magicpig" width="100%" height="150px" style="border-radius:15px"></a>
     <td width="67%" valign="top">
         <p><a href="https://arxiv.org/abs/2410.16179" target="_blank">
         <heading>MagicPIG: LSH Sampling for Efficient LLM Generation</heading></a><br>
         Zhuoming Chen, <strong>Ranajoy Sadhukhan</strong>, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, Beidi Chen<br>
         International Conference on Learning Representations (<strong>ICLR</strong> spotlight), 2025<br>
         </p>

         <div class="paper" id="magicpig">
         <a href="javascript:toggleblock('magicpigabs')">abstract</a> / 
         <a shape="rect" href="javascript:togglebib('magicpig')" class="togglebib">bibtex</a> / 
         <a href="https://arxiv.org/abs/2410.16179" target="_blank">paper</a>
         / <a href="https://www.lsh-ai.com/" target="_blank">blog</a>
         <br>

         <p align="justify"> <i id="magicpigabs">Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by up to $5\times$ across various GPU hardware and achieve 54ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens.</i></p>

<pre xml:space="preserve">
  @misc{chen2024magicpiglshsamplingefficientllm,
    title={MagicPIG: LSH Sampling for Efficient LLM Generation}, 
    author={Zhuoming Chen and Ranajoy Sadhukhan and Zihao Ye and Yang Zhou and Jianyu Zhang and Niklas Nolte and Yuandong Tian and Matthijs Douze and Leon Bottou and Zhihao Jia and Beidi Chen},
    year={2024},
    eprint={2410.16179},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2410.16179}, 
}
</pre>
        </div>
      </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
     <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2408.11049" target="_blank"><img src="images/MagicDec.png" alt="magicdec" width="100%" height="150px" style="border-radius:15px"></a>
     <td width="67%" valign="top">
         <p><a href="https://arxiv.org/abs/2408.11049" target="_blank">
         <heading>MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding</heading></a><br>
         <strong>Ranajoy Sadhukhan*</strong>, Jian Chen*, Zhuoming Chen, Vashisth Tiwari, Ruihang Lai, Jinyuan Shi, Ian En-Hsu Yen, Avner May, Tianqi Chen, Beidi Chen<br>
         International Conference on Learning Representations (<strong>ICLR</strong>), 2025<br>
         </p>
         <div class="paper" id="magicdec">
         <a href="javascript:toggleblock('magicdecabs')">abstract</a> / 
         <a shape="rect" href="javascript:togglebib('magicdec')" class="togglebib">bibtex</a> / 
         <a href="https://arxiv.org/abs/2408.11049" target="_blank">paper</a> /
          <a href="https://infini-ai-lab.github.io/MagicDec/" target="_blank">blog</a>
         <br>

         <p align="justify"> <i id="magicdecabs">Large Language Models (LLMs) have become more prevalent in long-context applications such as
          interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context
          requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to
          reduce latency without sacrificing performance but the conventional wisdom suggests that its efficacy is
          limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a
          high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting
          strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec
          first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights
          to deploy speculative decoding more effectively for high throughput inference. Then, it leverages draft
          models with sparse KV cache to address the KV bottleneck that scales with both sequence length and
          batch size. This finding underscores the broad applicability of speculative decoding in long-context serving,
          as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long
          sequences, we demonstrate up to 2x speedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving batch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. </i></p>

<pre xml:space="preserve">
  @misc{chen2024magicdecbreakinglatencythroughputtradeoff,
    title={MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding}, 
    author={Jian Chen and Vashisth Tiwari and Ranajoy Sadhukhan and Zhuoming Chen and Jinyuan Shi and Ian En-Hsu Yen and Beidi Chen},
    year={2024},
    eprint={2408.11049},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2408.11049}, 
}
</pre>
        </div>
      </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
     <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2408.11049" target="_blank"><img src="images/mosaics.png" alt="mosaics" width="90%" height="250px" style="border-radius:15px"></a>
     <td width="67%" valign="top">
         <p><a href="https://arxiv.org/abs/2405.06394" target="_blank">
         <heading>Memory Mosaics</heading></a><br>
         Jianyu Zhang, Niklas Nolte, <strong>Ranajoy Sadhukhan</strong>, Beidi Chen, Leon Bottou<br>
         International Conference on Learning Representations (<strong>ICLR</strong>), 2025<br>
         </p>

         <div class="paper" id="mosaics">
         <a href="javascript:toggleblock('mosaicsabs')">abstract</a> / 
         <a shape="rect" href="javascript:togglebib('mosaics')" class="togglebib">bibtex</a> / 
         <a href="https://arxiv.org/abs/2405.06394" target="_blank">paper</a> 
         <br>

         <p align="justify"> <i id="mosaicsabs">    Memory Mosaics are networks of associative memories working in concert to achieve a prediction task of interest. Like transformers, memory mosaics possess compositional capabilities and in-context learning capabilities. Unlike transformers, memory mosaics achieve these capabilities in comparatively transparent ways. We demonstrate these capabilities on toy examples and we also show that memory mosaics perform as well or better than transformers on medium-scale language modeling tasks. 
         </i></p>

<pre xml:space="preserve">
  @misc{zhang2024memorymosaics,
    title={Memory Mosaics}, 
    author={Jianyu Zhang and Niklas Nolte and Ranajoy Sadhukhan and Beidi Chen and LÃ©on Bottou},
    year={2024},
    eprint={2405.06394},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2405.06394}, 
}
</pre>
        </div>
      </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
     <td width="40%" valign="top" align="center"><a href="https://ieeexplore.ieee.org/document/9898007" target="_blank"><img src="images/HSD.png" alt="HSD-CNN" width="90%" height="250px" style="border-radius:15px"></a>
     <td width="67%" valign="top">
         <p><a href="https://ieeexplore.ieee.org/document/9898007" target="_blank">
         <heading>Taxonomy Driven Learning Of Semantic Hierarchy Of Classes</heading></a><br>
         <strong>Ranajoy Sadhukhan</strong>, Ankita Chatterjee, Jayanta Mukhopadhyay, Amit Patra</a> <br>
         IEEE International Conference on Image Processing (<strong>ICIP</strong>), 2022<br>
         </p>

         <div class="paper" id="hsdcnn">
         <a href="javascript:toggleblock('hsdcnnabs')">abstract</a> / 
         <a shape="rect" href="javascript:togglebib('hsdcnn')" class="togglebib">bibtex</a> / 
         <a href="https://ieeexplore.ieee.org/document/9898007" target="_blank">paper</a>
         <br>

         <p align="justify"> <i id="hsdcnnabs">Standard pre-trained convolutional neural networks are deployed on different task-specific limited class applications. These applications require classifying images of a much smaller subset of classes than that of the original large domain dataset on which the network is pre-trained. Therefore, a computationally inefficient and over-represented network is obtained. Hierarchically Self Decomposing CNN (HSD-CNN) addresses this issue by dissecting the network into sub-networks in an automated hierarchical fashion such that each sub-network is useful for classifying images of closely related classes. However, visual similarities are not always well-aligned with the semantic understanding of humans. In this paper, we propose a method that aids the pre-trained network to learn the hierarchy of classes derived from standard taxonomy, WordNet and, produce sub-networks corresponding to semantically meaningful classes upon decomposition. Experimental results show that the cluster of classes obtained for each sub-network is semantically closer according to WordNet hierarchy without degradation in overall accuracy.</i></p>

         
<pre xml:space="preserve">
@INPROCEEDINGS{9898007,
  author={Sadhukhan, Ranajoy and Chatterjee, Ankita and Mukhopadhyay, Jayanta and Patra, Amit},
  booktitle={2022 IEEE International Conference on Image Processing (ICIP)}, 
  title={Taxonomy Driven Learning Of Semantic Hierarchy Of Classes}, 
  year={2022},
  volume={},
  number={},
  pages={171-175},
  doi={10.1109/ICIP46576.2022.9898007}}
</pre>
        </div>
      </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
     <td width="40%" valign="top" align="center"><a href="https://ieeexplore.ieee.org/document/9190672" target="_blank"><img src="images/TuckerDecomp.png" alt="kdtucker" width="90%" height="250px" style="border-radius:15px"></a>
     <td width="67%" valign="top">
         <p><a href="https://ieeexplore.ieee.org/document/9190672" target="_blank">
         <heading>Knowledge Distillation Inspired Fine-Tuning Of Tucker Decomposed CNNS and Adversarial Robustness Analysis</heading></a><br>
         <strong>Ranajoy Sadhukhan</strong>, Avinab Saha, Jayanta Mukhopadhyay, Amit Patra <br>
         IEEE International Conference on Image Processing (<strong>ICIP</strong>), 2020<br>
         </p>

         <div class="paper" id="kdtucker">
         <a href="javascript:toggleblock('kdtuckerabs')">abstract</a> / 
         <a shape="rect" href="javascript:togglebib('kdtucker')" class="togglebib">bibtex</a> / 
         <a href="https://ieeexplore.ieee.org/document/9190672" target="_blank">paper</a>
         <br>

         <p align="justify"> <i id="kdtuckerabs">The recent works in Tensor decomposition of Convolutional Neural Networks have paid little attention to fine-tuning the decomposed models more effectively. We propose to improve the accuracy as well as the adversarial robustness of decomposed networks over existing non-iterative methods by distilling knowledge from the computationally intensive undecomposed(Teacher) model to the decomposed(Student) model. Through a series of experiments, we demonstrate the effectiveness of Knowledge Distillation with different loss functions and compare it to the existing fine-tuning strategy of minimizing Cross-Entropy loss with ground truth labels. Finally, we conclude that the Student networks obtained by the proposed approach are superior not only in terms of accuracy but also adversarial robustness, which is often compromised in the existing methods.</i></p>

<pre xml:space="preserve">
@INPROCEEDINGS{9190672,
  author={Sadhukhan, Ranajoy and Saha, Avinab and Mukhopadhyay, Jayanta and Patra, Amit},
  booktitle={2020 IEEE International Conference on Image Processing (ICIP)}, 
  title={Knowledge Distillation Inspired Fine-Tuning Of Tucker Decomposed CNNS and Adversarial Robustness Analysis}, 
  year={2020},
  volume={},
  number={},
  pages={1876-1880},
  doi={10.1109/ICIP40778.2020.9190672}}
</pre>
        </div>
      </td>
  </tr>
</table>


<!-- <a href="https://info.flagcounter.com/Bdj6"><img src="https://s05.flagcounter.com/countxl/Bdj6/bg_FFFFFF/txt_000000/border_CCCCCC/columns_4/maxflags_12/viewers_0/labels_0/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a> -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
     <tr><td><br><p align="right"><font size="2">
     Template: <a href="https://jonbarron.info">this</a>, <a href="https://people.eecs.berkeley.edu/~pathak/">this</a>, <a href="http://saurabhg.web.illinois.edu/">this</a> and <a href="https://homes.cs.washington.edu/~kusupati/">this</a>
     </font></p></td></tr>
</table>


  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('hsdcnnabs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('kdtuckerabs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('kineticsabs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('magicpigabs');
</script>
</body>

</html>